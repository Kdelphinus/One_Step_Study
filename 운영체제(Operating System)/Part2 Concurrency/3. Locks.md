## 기본 개념

```c
lock_t mutex; // 전역 변수로 선언된 lock
...
lock(&mutex);
balance = balance + 1;
unlock(&mutex);
```

- 락은 하나의 변수이기에 사용하기 위해 먼저 선언(mutex와 같이)해야 한다.
- 락은 락의 상태를 나타낸다.
	- 사용 가능(available): unlocked, free로도 이야기함
	- 사용 중(acquired): 임계 영역에서 정확히 하나의 쓰레드가 락을 획득한 상태
- 락 사용자는 이 정보를 알 수 없다.
- 쓰레드가 lock을 호출했을 때, 사용 가능하다면 쓰레드는 임계 영역에 진입한다.
- 만약 하나의 쓰레드가 락을 획득했는데 다른 쓰레드가 락을 호출하면 기존의 쓰레드가 락을 반환할 때까지 lock() 함수는 리턴하지 않는다.
- lock 사용자가 unlock()을 호출하면 락은 이제 다시 사용 가능 상태로 돌아온다.
- 이때, 대기 중인 쓰레드가 있다면 락을 획득하고 임계 영역으로 들어간다.
- 락을 통해 프로세스들의 혼란스러운 실행 순서에 어느 정도 질서를 부여할 수 있다.

## pthread 락

- POSIX 라이브러리는 쓰레드 간의 **상호 배제(mutual exclusion)** 기능을 제공하기에 이 안에 락을 **mutex** 라고 부른다. 
- 상호 배제는 말 그대로 하나의 쓰레드가 락을 얻었을 때, 다른 쓰레드들이 락을 얻지 못하도록 제한하는 데에서 이름을 따왔다.
- 락의 개수들을 통해 여러 전략도 사용할 수 있다.
	- 하나의 락이 임의의 임계 영역을 진입할 때마다 사용되는 것(**거친(coarse-grained) 락 사용 전략**)
	- 서로 다른 데이터와 자료 구조를 보호하기 위해 여러 락을 사용하여 한 번에 여러 쓰레드가 서로 다른 락으로 보호된 코드 내에서 각자가 진입이 가능하도록 하는 것(**세밀한(fine-grained) 락 사용 전략**)

## 락 구현

- 하드웨어와 운영체제의 도움이 필요하다.
- 정교한 락 라이브러리를 제작하는데, 사용되는 하드웨어 명렁어들과 운영체제가 관여하는 부분을 이어서 살핀다.

## 락의 평가

- 먼저 락을 만들기 전에 어떤 목적으로 구현하고, 어떤 방법으로 평가할지를 정해야 한다.
- 평가 기준
	- 상호 배제를 제대로 지원하는가
	- 쓰레드들에게 공정한 락 획득의 기회가 주어지는가
	- 성능은 준수한가
		- 단일 쓰레드일 때, 락을 획득하고 해제하는 과정에서 발생하는 부하는 얼마나 되는가
		- 여러 쓰레드가 단일 CPU 상에서 락을 획득하려 할 때 성능은 어떻게 되는가
		- 멀티 CPU 상황에서 락 경쟁 시의 성능은 어떻게 되는가

## 인터럽트 제어

- 초기 단일 프로세스 시스템에서는 상호 배제를 위해 임계 영역 내에서 인터럽트를 비활성화하는 방법을 사용했었다.
- 임계 영역에 진입하기 전에 특별한 하드웨어 명령어를 사용하여 인터럽트를 막았다.
- 이는 단순한 장점이 있다.
- 허나 많은 단점이 있다.
	- 쓰레드가 인터럽트를 활성/비활성화하는 **특권(privileged)** 연산을 실행할 수 있도록 허가해야 한다.
		- 그러면서 이 특권을 다른 목적으로 사용하지 않음을 신뢰할 수 있어야 한다.
		- 그러나 운영체제가 잘 알지 못하는 프로그램을 신뢰하는 것은 좋지 않은 경우
		- 예를 들어 탐욕적 기법을 사용한 프로그램이 lock을 계속 호출하여 lock을 무한정 얻어 무한 반복문에 들어갈 수 있다.
		- 이는 운영체제가 응용 프로그램을 너무 많이 신뢰해야 한다.
	- 멀티프로세스에서 적용할 수 없다.
		- 특정 프로세서에서 인터럽트 비활성화는 다른 프로ㅔ써에서 실행 중인 프로그램에게 전혀 영향을 주지 않는다.
	- 장시간동안 인터럽트를 중지시키는 것은 중요한 인터럽트의 시점을 놓칠 수 있다.
		- CPU가 저장 장치에서 읽기 요청을 마친 사실을 모르고 지나갔을 경우, 읽기 결과를 기다리는 프로세스를 영원히 깨우지 않을 수 있다.
	- 비효율적이다.
		- 인터럽트를 비활성화 시키는 코드는 최신 CPU들에서 느리게 실행되는 경향이 있다.

## Test-And-Set (Atomic Exchange)

- 멀티프로세서에선 인터럽트를 중지시키는 것이 의미가 업식에 락 지원을 위한 하드웨어를 설계하기 시작
- 이렇게 설계된 기법 중 가장 기본적인 기법이 **Test-And-Set** 명령어 혹은 **원자적 교체(atomic exchange)** 라고 불리는 기법이다. 
	- 임계 영역에 진입하는 첫 쓰레드가 lock()을 호출하여 플래그 값이 1인지 검사
	- 아니면 플래그를 1로 설정하고 이 쓰레드가 락을 보유하고 있다고 표시
	- 임계 영역에서 나오면 unlock()을 호출하여 플래그 값을 초기화하여 락을 더 이상 보유하고 있지 않다고 표시
- 이미 임계 영역에 쓰레드가 있는데 다른 쓰레드가 lock()을 호출하면 그 쓰레드는 while 문으로 **spin-wait** 을 하며 처음 쓰레드가 unlock() 을 호출하여 플래그를 초기화하기를 기다린다.
- 그러나 이 방법도 두 가지 문제점이 있다.
	- 정확성
		- 병행 프로그래밍에 익숙해지면 쉽게 알아볼 수 있다.
		- 적시에 인터럽트가 발생하면 두 쓰레드에 대해 플래그 1을 줄 수 있다.
	- 성능
		- 락을 얻기 위해 대기하는 부분에서 성능 문제가 생긴다.
		- spin-wait이라는 방법은 플래그의 값을 무한히 검사하는데, 이는 다른 쓰레드가 락을 해제할 때까지 시간을 낭비한다.
		- 단일 프로세스에서 매우 손해가 크며 이 경우, 락을 소유한 쓰레드조차 문맥 교환 없이는 실행이 불가능해진다.

## 진짜 돌아가는 스핀 락의 구현

- 앞선 예제는 하드웨어의 지원이 필요하다.
- 그리고 이를 지원해주는 시스템들도 많이 존재한다.
- 간단하게 Test-And-Set을 코드로 구현하면 다음과 같다.

```c
int TestAndSet(int *old_ptr, int new)
{
	int old = *old_ptr; // old_ptr의 이전 값을 가져옴
	*old_ptr = new;     // old_ptr에 'new' 값을 지정
	return old;         // old의 값을 반환함
}
```

- 이 코드의 핵심은 각 동작들이 원자적으로 수행된다는 것이다.
- 이름이 test and set인 이유는 검사(test)하는 동시에 메모리에 새로운 값을 설정(set)하기 때문이다. 그리고 이 동작을 원자적 연산으로 만듦으로써 오직 하나의 쓰레드만 락을 획득할 수 있도록 만들었다.
- 이것만으로 간단한 **스핀 락(spin lock)** 을 만들 수 있다.
- 락을 획득할 때까지, CPU 사이클을 소모하면서 회전한다. 그렇기에 스핀 락이라고 부른다.
- 단일 프로세서에선 이 방식을 제대로 사용하기 위해서 **선점형 스케줄러(preemptive scheduler)** 를 사용한다.
	- while문을 회전하며 대기하는 쓰레드가 CPU를 영원히 독점하는 것을 막기 위해서

## 스핀 락 평가

- 락에서 가장 중요한 측면은 **상호 배제의 정확성** 이다.
	- 상호 배제가 가능하다면 스핀 락은 임의의 시간에 단 하나의 쓰레드만이 임계 영역에 진입할 수 있도록 한다.
- 대기 중인 쓰레드들에 있어서 스핀 락이 얼마나 공정한지도 중요하다.
	- 이에 대해 스핀 락은 어떠한 공정성도 보장해주지 않는다.
	- while 문을 회전 중인 쓰레드는 경쟁에 밀려서 계속 그 상태에 남아있다가 기아 상태가 될 수 있다.
- 스핀 락을 사용할 때 지불되는 비용은 어떠한가
	- 단일 프로세서를 사용하며 락을 획득하기 위해 경쟁하는 경우
		- 스핀 락이 갖는 오버헤드가 상당히 클 수 있다.
		- 스케줄러가 락을 획득하려고 시도하는 쓰레드들을 하나씩 깨우며 오버헤드가 상당히 늘어날 수 있다.
		- 쓰레드는 할당받은 기간 동안 CPU 사이클을 낭비하면서 락을 획득하기 위해 대기한다.
	- 멀티 프로세서에 쓰레드가 퍼져있는 경우
		- 단일 프로세서와 다르게 꽤나 효율적으로 동작한다.
		- 쓰레드가 고를 수 있는 CPU의 개수가 많기 떄문이다.

## Compare-And-Swap

```c
int CompareAndSwap(int *ptr, int expected, int new)
{
	int actual = *ptr;
	if (actual == expected)
		*ptr = new;
	return actual;
}
```

- ptr이 가리키고 있는 주소 안에 값이 expected 변수와 일치하는지 검사하는 함수이다.
- 일치한다면 ptr이 가리키고 있는 주소의 값을 새로운 값으로 변경하고, 불일치한다면 아무 것도 하지 않는다.
- 그리고 원래의 메모리 값을 반환하여 CompareAndSwap을 호출한 코드가 락 획득의 성공 여부를 알 수 있도록 한다.
- 이는 락을 얻지 못한 쓰레드가 대기하지 않도록 만들어준다.
- 그렇기에 TestAndSet보다 더 강하다. **대기없는 동기화(wait-free synchronozation)** 를 다룰 때 강력하게 동작한다.
- 만약 단순 스핀 락과 동일하게 사용하면 앞서 분석한 스핀 락과 다를 바 없다.

## Load-Linked 그리고 Store-Conditional

- MIPS란 구조에선 **load-linked** 와 **store-conditional** 명령어를 앞뒤로 사용하여 락이나 기타 병행 연산을 위한 자료 구조를 만들 수 있다.

```c
int LoadLinked(int *ptr)
{
	return *ptr;
}

int StoreConditional(int *ptr, int value)
{
	if (no one has updated *ptr since the LoadLinked to this address)
	{
		*ptr = value;
		return 1; // 성공
	}
	else
		return 0; // 실패
}
```

- load-linked는  일반 로드 명령어와 같이 메모리 값을 레지스터에 저장한다.
- 차이는 store-conditional 명령어에서 나타나는데 동일한 주소에 다른 스토어가 없었던 경우에만 저장한다.
- 만약 다른 주소가 저장되어있다면 갱신하지 않고 실패했다고 알린다.

```c
void lock(lock_t *lock)
{
	while (1)
	{
		while (LoadLinked(&lock->flag) == 1); // 0이 될 때까지 스핀
		if (StoreConditional(&lock->flag, 1) == 1)
			return ; // 1로 변경하는 것이 성공하였다면 완료, 아니면 처음부터 시도
	}
}

void unlock(lock_t *lock)
{
	lock->flag = 0;
}
```

- load-linked와 store-conditional 함수를 이용해서 lock과 unlock 함수를 만들면 다음과 같다.
- 위 함수는 load-linked 함수를 통해서 락을 획득할 수 있을 때까지 기다린다.
- 만약 성공하면 flag를 1로 바꾸고 임계 영역으로 들어간다.
- 그리고 store-conditional로 들어가는데 이 때, 명령어가 실패할 수도 있다는 것이 중요한 관점이다.
	- 만약 인터럽트의 순서를 통해 각기 다른 두 개의 쓰레드가 모두 load-linked를 통과했다면 어떻게 되는가
	- 둘 중 먼저 store-conditional 명령어를 실행하는 함수는 임계 영역에 무사히 들어간다.
	- 허나 나중에 들어온 쓰레드는 이미 ptr 값이 갱신(첫 쓰레드가 갱신함)되어 있기에 실패하고 다시 임계 영역에 들어가기 위해 대기한다.

- 아래는 위 코드를 조금이나마 줄일 코드이다.

```c
void lock(lock_t *lock)
{
	while (LoadLinked(&lock->flag) || !StoreConditional(&lock->flag, 1));
}
```

## Fetch-And-Add

- **Fetch-And-Add** 명령어는 원자적으로 특정 주소의 예전 값을 반환하면서 값을 증가시킨다.

```c
int FetchAndAdd(int *ptr)
{
	int old = *ptr;
	*ptr = old + 1;
	return old;
}
```

- 이 명령어를 이용하여  **티켓 락** 이라는 것을 만들 수 있다.
	- 이 방법은 티켓(ticket)과 차례(turn) 조합을 사용하여 락을 만든다.
	- 하나의 쓰레드가 락 획득을 원하면, 티켓 변수에 원자적 동작인 fetch-and-add 명령어를 실행한다.
	- 결과 값은 해당 쓰레드의 "차례"를 나타낸다. 전역 공유 변수인 lock->turn을 사용하여 어느 쓰레드의 차례인지 판단한다.
	- 만약 쓰레드가 자기 차례라면 그 쓰레드가 임계 영역에 진입할 차례인 것이다.
	- 모든 동작이 끝나고 언락할 때, 차례 변수의 값을 증가시켜 대기 중인 (만약 있다면) 다음 쓰레드에게 임계 영역 진입 차례를 넘겨준다.
	- 이는 후순위 쓰레드가 자신의 순서를 보장받을 수 있다는 것을 의미한다.

## 요약: 과도한 스핀

- 지금까지 소개한 하드웨어 기반의 락은 간단하고 제대로 동작하는 장점이 있다.
- 그러나 임계 영역 내의 쓰레드가 인터럽트에 걸리면 락을 얻기 위해 대기하는 쓰레드는 비효율적으로 오랜 시간 기다려야 한다.
	- 임계 영역 내의 쓰레드가 타이머에 의해 다시 실행될 때까지 기다려야 하므로
	- 그렇기에 대기하는 쓰레드는 과도한 스핀을 일으킨다.
- 그렇기에 만약 N개의 쓰레드가 하나의 락을 획득하기 위해 경쟁하게 되면 N - 1개의 쓰레드에 할당된 CPU 시간 동안 계속 스핀되며 낭비되는 것이다.

## 간단한 접근법: 무조건 양보!

- 위와 같은 무한한 스핀을 해결하기 위해서 우리는 어떤 방법을 사용해야 하는가?

```c
void init()
{
	flag = 0;
}

void lock()
{
	while (TestAndSet(&flag, 1) == 1)
		yield(); // CPU를 양보함
}

void unlock()
{
	flag = 0;
}
```

- 첫번째 방법은 위와 같이 락이 해제되기를 기다리며 스핀해야 하는 경우, 자신에게 할당된 CPU를 다른 쓰레드에게 양보하는 것이다.
- 이때, CPU 시간을 포기하고 다른 쓰레드가 실행될 수 있도록 yield() 기법이 있다고 가정한다.
	- 이는 실행 중(runnung)인 쓰레드를 준비(ready) 상태로 변환하여 다른 쓰레드가 실행 중(running)인 상태로 전이하는 것이다.
	- 결과적으로 양보(yield) 동작은 스케줄 대상에서 자신을 빼는 것(deschedule)이다.
- 단일 CPU 시스템에서 두 개의 쓰레드를 실행하는 예를 생각할 때, 이 접근은 잘 동작되는 것을 확인할 수 있다.
- 허나 쓰레드가 많아질수록 N - 1개의 쓰레드가 실행하고 양보하는(run-and-yield) 패턴으로 동작하게 된다.
- 이는 99개의 시간 간격을 소비하는 것보단 괜찮지만 여전히 비효율적이다. 특히 문맥 교환 비용이 상당하다.

## 큐의 사용: 스핀 대신 잠자기

- 위와 같은 방법은 스케줄러의 선택에 따라 효율성이 변하는 운적인 요소가 너무 많다.
- 그렇기에 운영체제는 다음으로 락을 획득할 쓰레드를 명시적으로 지원해야 하며 이를 위해 적절한 동작과 큐를 이용한 대기 쓰레드 관리가 필요하다.
- 이러한 방법의 간단한 설명을 위해 Solaris 방식을 사용한다.
	- 쓰레드를 재우는 함수인 park()
	- 쓰레드를 깨우는 함수인 unpark()
	- 두 함수는 이미 사용 중인 락을 요청하는 프로세스를 재우고 해당 락이 해제되면 깨우도록 락을 제작하는데 사용될 수 있다.

```c
typedef struct __lock_t
{
	int flag;
	int guard;
	queue_t *q;
} lock_t;

void lock_init(lock_t *m)
{
	m->flag = 0;
	m->guard = 0;
	queue_init(m->q);
}

void lock(lock_t *m)
{
	while (TestAndSet(&m->guard, 1) == 1); // 회전하면서 guard 락을 획득
	if (m->flag == 0)
	{
		m->flag = 1; // 락을 획득
		m->guard = 0;
	}
	else
	{
		queue_add(m->q, gettid());
		m->guard = 0;
		park();
	}
}

void unlock(lock_t *m)
{
	while (TestAndSet(&m->guard, 1) == 1); // 회전하면서 guard 락을 획득
	if (queue_empty(m->q))
		m->flag = 0; // 락을 포기, 누구도 락을 원하지 않음
	else
		unpark(queue_remove(m->q)); // 락을 획득함 (다음 쓰레드를 위하여)
	m->guard = 0;
}
```

- 위를 보면 guard 변수를 사용하여 flag와 큐의 삽입과 삭제를 스핀 락으로 보호하고 있다.
	- 허나 이 방법은 회전 대기를 완전히 배제하지는 못한다.
	- 쓰레드가 락을 획득하거나 해제하는 과정에서 인터럽트에 걸릴 수 있다.
	- 이때, 다른 쓰레드는 락의 해제를 기다린다. 물론 이때 회전 대기 시간은 꽤나 짧다.
	- 왜냐하면 임계 영역에 진입하는 것이 아니기 때문이다.
- 또한 lock() 함수에도 동작을 추가했다.
	- 다른 쓰레드가 이미 lock()을 얻었다면 gettid()로 현재 실행 중인 쓰레드 id를 얻는다.
	- 그 다음 락 소유자가 가진 큐에 자기 자신을 추가하고 guard 변수를 0으로 변경한다.
	- 그리고 CPU를 양보한다.
- 그러나 위 코드는 park() 직전에 경쟁 조건이 발생할 수 있다.
	- 한 쓰레드가 락을 얻지 못하여 park()를 수행하려 한다.
	- 근데 park()를 수행하기 직전에 락 소유자에게 CPU가 할당되어 락이 할당된 쓰레드를 해제했다.
	- 그리고 다시 쓰레드의 차례가 돌아오면 park()로 잠들게 되고 다시 깨어날 수 없게 된다.
	- 이를 **깨우기/대기경쟁(wakeup/waiting race)** 라고 부른다.

```c
queue_add(m->q, gettid());
setpark(); // 새로 추가된 코드
m->guard = 0;
```
- 이러한 문제의 해결을 위해 Solaris는 setpark() 함수를 도입했다.
	- 만약 park() 수행 직전에 다른 쓰레드가 unpark()를 먼저 호출하면, 추후 park() 문은 잠을 자지 않고 바로 리턴된다.
- 아니면 guard 변수의 역할을 커널에서 담당하는 것도 방법이 될 수 있다.
	- 이때는 커널이 락 해제와 실행 중인 쓰레드를 큐에서 제거하는 동작을 실행할 때, 원자적으로 처리하도록 주의를 기울여야 한다.

## 다른 운영체제, 다른 지원

- Linux의 경우, **futex** 라는 것을 지원한다.
- 31.15 참고

## 2단계 락

- **2단계 락(two-phase lock)** 은 락이 곧 해제될 것 같은 경우엔, 회전 대기가 유용할 수 있다는 것에서 착안되었다.
	- 1단계에선 곧 락을 획득할 수 있을 것이라는 기대로 회전하며 기다린다.
	- 만약 1단계에서 락을 획득하지 못했다면 2단계로 넘어간다.
	- 2단계에선 호출자는 잠에 빠지고 락이 해제된 후에 깨어나도록 한다.
- Linux의 락은 이러한 형태를 가지지만 한 번만 회전한다.\
- 일반화된 방법은 futex가 잠재우기 전에 일정 시간 동안 반복문 내에서 회전하도록 하는 것이다.